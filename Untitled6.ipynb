{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9673f7b6-6c93-41ea-a786-e606696e67b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\parth\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 837ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "['i', 'bye', 'bye']\n",
      "\"I bid you farewell, goodbye.\"\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from cvzone.ClassificationModule import Classifier\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from gtts import gTTS \n",
    "import platform\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model(\"Model/keras_model.h5\")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "detector = HandDetector(maxHands=1)\n",
    "classifier = Classifier(\"Model/keras_model.h5\", \"Model/labels.txt\")\n",
    "offset = 20\n",
    "imgSize = 300\n",
    "\n",
    "labels = [\"peace\", \"good luck\", \"again\", \"no\", \"help\", \"yes\", \"hello\", \"busy\", \"happy\", \"i\", \"you\", \"bye\"]\n",
    "\n",
    "# Initialize an empty list to store the predictions\n",
    "predictions = []\n",
    "\n",
    "# Set the duration and interval for capturing gestures\n",
    "duration = 10  # Total duration in seconds\n",
    "interval = 3  # Capture interval in seconds\n",
    "  \n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Google Generative AI configuration\n",
    "genai.configure(api_key=\"\")\n",
    "\n",
    "generation_config = {\n",
    "  \"temperature\": 0.9,\n",
    "  \"top_p\": 1,\n",
    "  \"top_k\": 0,\n",
    "  #\"max_output_tokens\": 20,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "safety_settings = [\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "  },\n",
    "]\n",
    "\n",
    "genai_model = genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.0-pro\",\n",
    "  #safety_settings=safety_settings,\n",
    "  generation_config=generation_config,\n",
    ")\n",
    "\n",
    "def get_meaningful_sentence(predictions):\n",
    "    prompt = f\"I am giving you the following keywords: {predictions}, frame a logical and sensible sentence used in every day life.\"\n",
    "    chat_session = genai_model.start_chat(history=[])\n",
    "    response = chat_session.send_message(prompt)\n",
    "    return response.text.strip()\n",
    "\n",
    "def text_to_speech(text):\n",
    "    tts = gTTS(text=text, lang='en')\n",
    "    tts.save(\"output.mp3\")\n",
    "    if platform.system() == \"Windows\":\n",
    "        os.system(\"start output.mp3\")\n",
    "    elif platform.system() == \"Darwin\":  # macOS\n",
    "        os.system(\"afplay output.mp3\")\n",
    "    elif platform.system() == \"Linux\":\n",
    "        os.system(\"mpg321 output.mp3\")\n",
    "\n",
    "while True:\n",
    "    # Check if the total duration has passed\n",
    "    elapsed_time = time.time() - start_time\n",
    "    if elapsed_time > duration:\n",
    "        break\n",
    "\n",
    "    success, img = cap.read()\n",
    "    imgOutput = img.copy()\n",
    "    hands, img = detector.findHands(img)\n",
    "    if hands:\n",
    "        hand = hands[0]\n",
    "        x, y, w, h = hand['bbox']\n",
    "\n",
    "        imgWhite = np.ones((imgSize, imgSize, 3), np.uint8) * 255\n",
    "\n",
    "        imgCrop = img[max(0, y-offset):min(y + h + offset, img.shape[0]), max(0, x-offset):min(x + w + offset, img.shape[1])]\n",
    "                      \n",
    "        imgCropShape = imgCrop.shape \n",
    "\n",
    "        aspectRatio = h / w\n",
    "\n",
    "        if aspectRatio > 1:\n",
    "            k = imgSize / h\n",
    "            wCal = math.ceil(k * w)\n",
    "            imgResize = cv2.resize(imgCrop, (wCal, imgSize))\n",
    "            imgResizeShape = imgResize.shape\n",
    "            wGap = math.ceil((imgSize - wCal) / 2)\n",
    "            imgWhite[:, wGap: wCal + wGap] = imgResize\n",
    "            prediction, index = classifier.getPrediction(imgWhite, draw=False)\n",
    "        else:\n",
    "            k = imgSize / w\n",
    "            hCal = math.ceil(k * h)\n",
    "            imgResize = cv2.resize(imgCrop, (imgSize, hCal))\n",
    "            imgResizeShape = imgResize.shape\n",
    "            hGap = math.ceil((imgSize - hCal) / 2)\n",
    "            imgWhite[hGap: hCal + hGap, :] = imgResize\n",
    "            prediction, index = classifier.getPrediction(imgWhite, draw=False)\n",
    "\n",
    "        # Capture the prediction every interval seconds\n",
    "        if int(elapsed_time) % interval == 0:\n",
    "            predictions.append(labels[index])  # Append the label to predictions\n",
    "            # Wait to ensure the gesture is not captured multiple times within the same second\n",
    "            time.sleep(1)\n",
    "\n",
    "        cv2.rectangle(imgOutput, (x - offset, y - offset - 70), (x - offset + 400, y - offset + 60 - 50), (0, 255, 0), cv2.FILLED)\n",
    "        cv2.putText(imgOutput, labels[index], (x, y - 30), cv2.FONT_HERSHEY_COMPLEX, 2, (0, 0, 0), 2)\n",
    "        cv2.rectangle(imgOutput, (x - offset, y - offset), (x + w + offset, y + h + offset), (0, 255, 0), 4)\n",
    "\n",
    "       # cv2.imshow('ImageCrop', imgCrop)\n",
    "       # cv2.imshow('ImageWhite', imgWhite)\n",
    "\n",
    "    cv2.imshow('Image', imgOutput)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "# Release the video capture object and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Use the predictions to generate a meaningful sentence and convert it to speech\n",
    "print(predictions)\n",
    "text = get_meaningful_sentence(predictions)\n",
    "print(text)\n",
    "text_to_speech(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e87168-6ce0-4474-b3c9-1722a458f6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
